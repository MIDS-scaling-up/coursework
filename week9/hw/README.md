# Streaming Tweet Processing

## Spark Streaming Introduction

In addition to providing mechanisms for working with RDDs from static sources like flat files, Apache Spark provides a processing model to work with stream input data. A **DStream** is a Spark abstraction for incoming stream data and can be operated on with many of the functions used to manipulate RDDs. In fact, a DStream object contains a series of data from the input stream split up into RDDs containing some set of data from the stream for a particular duration. A Spark user can register functions to operate on RDDs in the stream as they become available, or on multiple, consecutive RDDs by combining them.

For more information, please consult the [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html).

## Part 1: Set up a 3-node Spark Cluster

Provision 3 VSes to comprise a Spark cluster. You may set up the cluster manually following the instructions from the previous assignment, [Apache Spark Introduction](../../week6/hw/apache_spark_introduction). 

***Note that we are using Spark 1.6.1. This homework will not work with Spark 2.0***


## Part 2: Build a Twitter popular topic and user reporting system

Design and build a system for collecting data about 'popular' hashtags and users related to tweets containing them. The popularity of hashtags is determined by the frequency of occurrence of those hashtags in tweets over a sampling period. Record popular hashtags and **both** the users who authored tweets containing them as well as other users mentioned in them. For example, if @solange tweets "@jayZ is performing #theblackalbum tonight at Madison Square Garden!! @beyonce will be there!", 'theblackalbum' is a popular topic, and all of the users related to the tweet—'beyonce', 'solange', and 'jayZ'—should be recorded.

The output of your program should be lists of hashtags that were determined to be popular during the program's execution, as well as lists of users, per-hashtag, who were related to them. Think of this output as useful to marketers who want to target people to sell products to: the ones who surround conversations about particular events, products, and brands are more likely to purchase them than a random user.

Your implementation should continually process incoming Twitter stream data for the duration of at least **30 minutes** and output a summary of data collected. Your program should also sample tweets over a **short** sampling duration in the range of a few minutes. The number of top most popular hashtags, _n_, to aggregate **at each sampling interval up to the total execution time** must be configurable as well. From tweets gathered during both short and long sampling periods you should determine:

- The top _n_ most frequently-occurring hashtags among all tweets during the sampling period
- The account names of users who authored tweets with popular hashtags in the period
- The account names of users who were mentioned in popular tweets

Your output should display these facts.

### Getting Started

Spark is written in Scala, a language that compiles to Java bytecode and runs on the Java Virtual Machine. It provides support for executing code written in languages other than Scala (like Jython, or Python on the JVM), but some features aren't implemented in such languages. You're welcome to implement this assignment in whichever of Spark's supported languages you wish. If you're not opposed to learning some Scala, I'd suggest you give it a try: it's a powerful language that fits well with Spark's paradigm.

#### Official Twitter Example

There is an official Spark Streaming Twitter example you can learn from, but not that fulfilling this assignment isn't about merely extending what is in that example. It's available at https://github.com/apache/spark/blob/branch-1.6/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala


#### Twitter4J Library

The tweet abstractions your Spark program will receive are generated by a library called Twitter4J. The documentation is available at http://twitter4j.org/en. Of particular interest to you are these classes:

* http://twitter4j.org/javadoc/twitter4j/Status.html
* http://twitter4j.org/javadoc/twitter4j/User.html

If you'd like to understand how a Twitter4J `Status` object ends up in your Spark program, you might consult: https://github.com/apache/spark/blob/f85aa06464a10f5d1563302fd76465dded475a12/external/twitter/src/main/scala/org/apache/spark/streaming/twitter/TwitterInputDStream.scala.

#### Building with SBT

The Scala Build Tool (SBT) can be used to build a bytecode package (JAR file) for execution in a Spark cluster. You bundled such a JAR and executed it on a Spark cluster in the previous assignment, [Apache Spark Introduction](../../week6/hw/apache_spark_introduction). You can follow pattern similiar to the one established there for building a project and executing it. For convenience, you might start with the following project structure and files.

    ./twitter_popularity
    ├── build.sbt
    ├── project
    │   └── plugins.sbt
    └── twitter_popularity.scala

##### `build.sbt`

    lazy val common = Seq(
      organization := "week9.mids",
      version := "0.1.0",
      scalaVersion := "2.10.6",
      libraryDependencies ++= Seq(
        "org.apache.spark" %% "spark-streaming" % "1.6.1" % "provided",
        "org.apache.spark" %% "spark-streaming-twitter" % "1.6.1",
        "com.typesafe" % "config" % "1.3.0"
      ),
      mergeStrategy in assembly <<= (mergeStrategy in assembly) { (old) =>
         {
          case PathList("META-INF", xs @ _*) => MergeStrategy.discard
          case x => MergeStrategy.first
         }
      }
    )

    lazy val twitter_popularity = (project in file(".")).
      settings(common: _*).
      settings(
        name := "twitter_popularity",
        mainClass in (Compile, run) := Some("twitter_popularity.Main"))

Note the specification of Spark library versions. Ensure that these version numbers match the version of Spark you have installed in your cluster or scary and terrible things may occur that prevent you from achieving Zen-like project execution bliss.

##### `project/plugins.sbt`

    addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "3.0.0")
    addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

##### `twitter_popularity.scala`

    object Main extends App {
      println(s"I got executed with ${args size} args, they are: ${args mkString ", "}")

      // your code goes here
    }

#### Building and Executing your code

From spark1 in the root of the project directory, execute the following where 'foof', 'goof' and 'spoof' are args to the Spark program:

    sbt clean assembly && $SPARK_HOME/bin/spark-submit \
      --master spark://spark1:7077 $(find target -iname "*assembly*.jar") \
      foof goof spoof

Note that the 'clean' build target is only necessary if you remove files or dependencies from a project; if you've merely changed or added files previously built, you can execute only the `package` target for a speedier build.

## Grading and Submission

This is a graded assignment. Please submit credentials to access your cluster and execute the program. The output can be formatted as you see fit, but must contain lists of popular hashtags and people __related__ to each hashtag.

When submitting credentials to your Spark system, please provide a short description of a particularly interesting decision or two you made about the processing interval, features about collection, or other features of your collection system that make for particularly useful output data.
